{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambda_min = 0.95\n",
    "\n",
    "x = torch.tensor([[1., 0.], [0., lambda_min]])\n",
    "y_true = torch.tensor([[1.], [1.]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(2, 1, bias=False)\n",
    "        self.fc2 = nn.Linear(1, 1, bias=False)\n",
    "\n",
    "        self.gradients = {\n",
    "            'fc1.weight': [],\n",
    "            # 'fc1.bias': [],\n",
    "            'fc2.weight': [],\n",
    "            # 'fc2.bias': []\n",
    "        }\n",
    "        self.loss = []\n",
    "        self.ntk = []\n",
    "\n",
    "    def init_layer1(self, weight, bias):\n",
    "        self.fc1.weight.data = weight.float()\n",
    "        # self.fc1.bias.data = bias.float()\n",
    "\n",
    "    def init_layer2(self, weight, bias):\n",
    "        self.fc2.weight.data = weight.float()\n",
    "        # self.fc2.bias.data = bias.float()\n",
    "\n",
    "    def get_layer1(self):\n",
    "        return self.fc1.weight.data\n",
    "    \n",
    "    def get_layer2(self):\n",
    "        return self.fc2.weight.data\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "    \n",
    "    def train_epoch(self, x, y_true, eta):\n",
    "        y_pred = self.forward(x)\n",
    "        loss = F.l1_loss(y_pred, y_true)\n",
    "        self.zero_grad()\n",
    "        loss.backward()\n",
    "        with torch.no_grad():\n",
    "            for param in self.parameters():\n",
    "                param -= eta * param.grad\n",
    "        \n",
    "        for name, param in self.named_parameters():\n",
    "            self.gradients[name].append(param.grad)\n",
    "        self.loss.append((y_pred - y_true))\n",
    "\n",
    "\n",
    "        return loss\n",
    "    \n",
    "    def train(self, x, y_true, eta, epochs=None):\n",
    "        if epochs is None:\n",
    "            epochs = 1\n",
    "            while True:\n",
    "                loss = self.train_epoch(x, y_true, eta)\n",
    "\n",
    "                theta1, theta2 = self.get_layer1().flatten().tolist()\n",
    "                theta3 = self.get_layer2().flatten().item()\n",
    "                ntk = [\n",
    "                        [ 1 - eta*(theta1**2+theta3**2), -eta*theta1*theta2],\n",
    "                        [-eta*theta1*theta2, 1 - eta*(theta2**2+theta3**2)]\n",
    "                ]\n",
    "\n",
    "\n",
    "                print(f'Epoch {epochs}: loss {loss}' + f' eigen {np.linalg.eigvals(ntk)}')\n",
    "                print(f' theta1 {theta1} theta2 {theta2} theta3 {theta3}')  \n",
    "                print(f' theta1*theta3 {theta1*theta3} theta2*theta3 {theta2*theta3}')\n",
    "                epochs += 1\n",
    "                if loss < 1e-6 or epochs > 10:\n",
    "                    break\n",
    "        else:\n",
    "            for epoch in range(epochs):\n",
    "                loss = self.train_epoch(x, y_true, eta)\n",
    "                print(f'Epoch {epoch}: loss {loss}')\n",
    "        return loss "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.05124835742444156\n",
      "Epoch 1: loss 1.0 eigen [-0.34894091  0.05387648]\n",
      " theta1 0.47306177020072937 theta2 0.4494086802005768 theta3 1.0\n",
      " theta1*theta3 0.47306177020072937 theta2*theta3 0.4494086802005768\n",
      "Epoch 2: loss 0.5499999523162842 eigen [-2.53452966 -0.92326011]\n",
      " theta1 0.9461235404014587 theta2 0.8988173604011536 theta3 1.4257556200027466\n",
      " theta1*theta3 1.3489409549442755 theta2*theta3 1.2814939029479788\n",
      "Epoch 3: loss 0.2831800580024719 eigen [0.55517776 0.68800945]\n",
      " theta1 0.27165305614471436 theta2 0.2580704092979431 theta3 0.5742444396018982\n",
      " theta1*theta3 0.15599525699196448 theta2*theta3 0.14819549756512984\n",
      "Epoch 4: loss 0.8516095280647278 eigen [-0.16553458  0.36579219]\n",
      " theta1 0.5433061122894287 theta2 0.5161408185958862 theta3 0.8187322020530701\n",
      " theta1*theta3 0.4448222097036165 theta2*theta3 0.4225811089784841\n",
      "Epoch 5: loss 0.5768628716468811 eigen [-2.17685183 -0.61796551]\n",
      " theta1 0.9306169748306274 theta2 0.8840861320495605 theta3 1.3077077865600586\n",
      " theta1*theta3 1.2169750642909776 theta2*theta3 1.1561263188709745\n",
      "Epoch 6: loss 0.15764755010604858 eigen [0.61565719 0.79086567]\n",
      " theta1 0.3119904398918152 theta2 0.29639095067977905 theta3 0.47015249729156494\n",
      " theta1*theta3 0.1466830844462308 theta2*theta3 0.13934894563671918\n",
      "Epoch 7: loss 0.8604676723480225 eigen [-0.04758804  0.46646511]\n",
      " theta1 0.5344015955924988 theta2 0.5076815485954285 theta3 0.7509438991546631\n",
      " theta1*theta3 0.40130561790870445 theta2*theta3 0.3812403616311286\n",
      "Epoch 8: loss 0.618257999420166 eigen [-1.86046943 -0.43582832]\n",
      " theta1 0.8896444439888 theta2 0.8451622724533081 theta3 1.2319053411483765\n",
      " theta1*theta3 1.0959577422727804 theta2*theta3 1.0411599175723296\n",
      "Epoch 9: loss 0.053427934646606445 eigen [-0.34176941 -2.28207043]\n",
      " theta1 0.30687713623046875 theta2 1.3987911939620972 theta3 1.1908718347549438\n",
      " theta1*theta3 0.3654513382671212 theta2*theta3 1.6657810355927012\n",
      "Epoch 10: loss 0.6085202693939209 eigen [-0.89560722  0.52652601]\n",
      " theta1 0.8702330589294434 theta2 0.8636030554771423 theta3 0.7074148058891296\n",
      " theta1*theta3 0.6156157504608757 theta2*theta3 0.6109255878556219\n"
     ]
    }
   ],
   "source": [
    "net = Net()\n",
    "net.init_layer1(torch.tensor([[0, 0]]), torch.tensor([0]))\n",
    "net.init_layer2(torch.tensor([[1]]), torch.tensor([0]))\n",
    "\n",
    "eta = 2/(1+lambda_min**2)\n",
    "print(1-eta)\n",
    "eta = eta*0.9\n",
    "loss = net.train(x, y_true, eta=eta)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
