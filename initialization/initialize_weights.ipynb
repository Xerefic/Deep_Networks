{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor([[1., 0.], [0., 0.5]])\n",
    "y_true = torch.tensor([[1.], [1.]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(2, 1, bias=False)\n",
    "        self.fc2 = nn.Linear(1, 1, bias=False)\n",
    "\n",
    "        self.gradients = {\n",
    "            'fc1.weight': [],\n",
    "            # 'fc1.bias': [],\n",
    "            'fc2.weight': [],\n",
    "            # 'fc2.bias': []\n",
    "        }\n",
    "        self.loss = []\n",
    "        self.ntk = []\n",
    "\n",
    "    def init_layer1(self, weight, bias):\n",
    "        self.fc1.weight.data = weight.float()\n",
    "        # self.fc1.bias.data = bias.float()\n",
    "\n",
    "    def init_layer2(self, weight, bias):\n",
    "        self.fc2.weight.data = weight.float()\n",
    "        # self.fc2.bias.data = bias.float()\n",
    "\n",
    "    def get_layer1(self):\n",
    "        return self.fc1.weight.data\n",
    "    \n",
    "    def get_layer2(self):\n",
    "        return self.fc2.weight.data\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "    \n",
    "    def train_epoch(self, x, y_true, eta):\n",
    "        y_pred = self.forward(x)\n",
    "        loss = F.l1_loss(y_pred, y_true)\n",
    "        self.zero_grad()\n",
    "        loss.backward()\n",
    "        with torch.no_grad():\n",
    "            for param in self.parameters():\n",
    "                param -= eta * param.grad\n",
    "        \n",
    "        for name, param in self.named_parameters():\n",
    "            self.gradients[name].append(param.grad)\n",
    "        self.loss.append((y_pred - y_true))\n",
    "\n",
    "\n",
    "        return loss\n",
    "    \n",
    "    def train(self, x, y_true, eta, epochs=None):\n",
    "        if epochs is None:\n",
    "            epochs = 1\n",
    "            while True:\n",
    "                loss = self.train_epoch(x, y_true, eta)\n",
    "                print(f'Epoch {epochs}: loss {loss}')\n",
    "                epochs += 1\n",
    "                if loss < 1e-6 or epochs > 1000:\n",
    "                    break\n",
    "        else:\n",
    "            for epoch in range(epochs):\n",
    "                loss = self.train_epoch(x, y_true)\n",
    "                print(f'Epoch {epoch}: loss {loss}')\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: loss 1.0\n",
      "Epoch 2: loss 0.96875\n",
      "Epoch 3: loss 0.9373047351837158\n",
      "Epoch 4: loss 0.9052725434303284\n",
      "Epoch 5: loss 0.8722571134567261\n",
      "Epoch 6: loss 0.8378521203994751\n",
      "Epoch 7: loss 0.8016366362571716\n",
      "Epoch 8: loss 0.763169527053833\n",
      "Epoch 9: loss 0.721984326839447\n",
      "Epoch 10: loss 0.6775836944580078\n",
      "Epoch 11: loss 0.6294331550598145\n",
      "Epoch 12: loss 0.5769545435905457\n",
      "Epoch 13: loss 0.5195194482803345\n",
      "Epoch 14: loss 0.4564414620399475\n",
      "Epoch 15: loss 0.3869679570198059\n",
      "Epoch 16: loss 0.41383740305900574\n",
      "Epoch 17: loss 0.3591017425060272\n",
      "Epoch 18: loss 0.3958168029785156\n",
      "Epoch 19: loss 0.33080804347991943\n",
      "Epoch 20: loss 0.37396302819252014\n",
      "Epoch 21: loss 0.31340306997299194\n",
      "Epoch 22: loss 0.34155625104904175\n",
      "Epoch 23: loss 0.2955520749092102\n",
      "Epoch 24: loss 0.30633029341697693\n",
      "Epoch 25: loss 0.2770194113254547\n",
      "Epoch 26: loss 0.26802048087120056\n",
      "Epoch 27: loss 0.25755587220191956\n",
      "Epoch 28: loss 0.2263418436050415\n",
      "Epoch 29: loss 0.23689627647399902\n",
      "Epoch 30: loss 0.18098655343055725\n",
      "Epoch 31: loss 0.214756578207016\n",
      "Epoch 32: loss 0.1413211226463318\n",
      "Epoch 33: loss 0.17923825979232788\n",
      "Epoch 34: loss 0.11615568399429321\n",
      "Epoch 35: loss 0.12597492337226868\n",
      "Epoch 36: loss 0.08861503005027771\n",
      "Epoch 37: loss 0.06797361373901367\n",
      "Epoch 38: loss 0.058335840702056885\n",
      "Epoch 39: loss 0.026939481496810913\n",
      "Epoch 40: loss 0.057971835136413574\n",
      "Epoch 41: loss 0.02677130699157715\n",
      "Epoch 42: loss 0.057610124349594116\n",
      "Epoch 43: loss 0.02660432457923889\n",
      "Epoch 44: loss 0.05725061893463135\n",
      "Epoch 45: loss 0.026438266038894653\n",
      "Epoch 46: loss 0.056893378496170044\n",
      "Epoch 47: loss 0.026273220777511597\n",
      "Epoch 48: loss 0.05653837323188782\n",
      "Epoch 49: loss 0.026109308004379272\n",
      "Epoch 50: loss 0.05618554353713989\n",
      "Epoch 51: loss 0.025946378707885742\n",
      "Epoch 52: loss 0.055834949016571045\n",
      "Epoch 53: loss 0.026026159524917603\n",
      "Epoch 54: loss 0.09479504823684692\n",
      "Epoch 55: loss 0.032103925943374634\n",
      "Epoch 56: loss 0.08796322345733643\n",
      "Epoch 57: loss 0.038143813610076904\n",
      "Epoch 58: loss 0.08117413520812988\n",
      "Epoch 59: loss 0.04414603114128113\n",
      "Epoch 60: loss 0.07442730665206909\n",
      "Epoch 61: loss 0.050110846757888794\n",
      "Epoch 62: loss 0.06772267818450928\n",
      "Epoch 63: loss 0.056038349866867065\n",
      "Epoch 64: loss 0.061059772968292236\n",
      "Epoch 65: loss 0.06192886829376221\n",
      "Epoch 66: loss 0.05443859100341797\n",
      "Epoch 67: loss 0.06778264045715332\n",
      "Epoch 68: loss 0.04785865545272827\n",
      "Epoch 69: loss 0.07359984517097473\n",
      "Epoch 70: loss 0.04131978750228882\n",
      "Epoch 71: loss 0.07938075065612793\n",
      "Epoch 72: loss 0.03482180833816528\n",
      "Epoch 73: loss 0.08512565493583679\n",
      "Epoch 74: loss 0.028364241123199463\n",
      "Epoch 75: loss 0.09083467721939087\n",
      "Epoch 76: loss 0.021947026252746582\n",
      "Epoch 77: loss 0.09650805592536926\n",
      "Epoch 78: loss 0.015569925308227539\n",
      "Epoch 79: loss 0.10214602947235107\n",
      "Epoch 80: loss 0.00923246145248413\n",
      "Epoch 81: loss 0.10774886608123779\n",
      "Epoch 82: loss 0.004105597734451294\n",
      "Epoch 83: loss 0.07606542110443115\n",
      "Epoch 84: loss 0.004079967737197876\n",
      "Epoch 85: loss 0.07559069991111755\n",
      "Epoch 86: loss 0.004054516553878784\n",
      "Epoch 87: loss 0.07511907815933228\n",
      "Epoch 88: loss 0.006415575742721558\n",
      "Epoch 89: loss 0.11458498239517212\n",
      "Epoch 90: loss 0.012615740299224854\n",
      "Epoch 91: loss 0.10762971639633179\n",
      "Epoch 92: loss 0.018777310848236084\n",
      "Epoch 93: loss 0.10071778297424316\n",
      "Epoch 94: loss 0.024900346994400024\n",
      "Epoch 95: loss 0.0938490629196167\n",
      "Epoch 96: loss 0.030985236167907715\n",
      "Epoch 97: loss 0.08702313899993896\n",
      "Epoch 98: loss 0.037032127380371094\n",
      "Epoch 99: loss 0.08023983240127563\n",
      "Epoch 100: loss 0.04304128885269165\n",
      "Epoch 101: loss 0.07349890470504761\n",
      "Epoch 102: loss 0.049012959003448486\n",
      "Epoch 103: loss 0.06679993867874146\n",
      "Epoch 104: loss 0.054947346448898315\n",
      "Epoch 105: loss 0.06014293432235718\n",
      "Epoch 106: loss 0.06084471940994263\n",
      "Epoch 107: loss 0.0535273551940918\n",
      "Epoch 108: loss 0.06670525670051575\n",
      "Epoch 109: loss 0.046953022480010986\n",
      "Epoch 110: loss 0.07252928614616394\n",
      "Epoch 111: loss 0.040419816970825195\n",
      "Epoch 112: loss 0.07831695675849915\n",
      "Epoch 113: loss 0.033927321434020996\n",
      "Epoch 114: loss 0.08406847715377808\n",
      "Epoch 115: loss 0.027475416660308838\n",
      "Epoch 116: loss 0.08978402614593506\n",
      "Epoch 117: loss 0.021063804626464844\n",
      "Epoch 118: loss 0.09546396136283875\n",
      "Epoch 119: loss 0.014692187309265137\n",
      "Epoch 120: loss 0.1011083722114563\n",
      "Epoch 121: loss 0.011627405881881714\n",
      "Epoch 122: loss 0.06526246666908264\n",
      "Epoch 123: loss 0.011554837226867676\n",
      "Epoch 124: loss 0.06485521793365479\n",
      "Epoch 125: loss 0.011482715606689453\n",
      "Epoch 126: loss 0.0644504725933075\n",
      "Epoch 127: loss 0.011411041021347046\n",
      "Epoch 128: loss 0.06404832005500793\n",
      "Epoch 129: loss 0.01133987307548523\n",
      "Epoch 130: loss 0.06364864110946655\n",
      "Epoch 131: loss 0.011269092559814453\n",
      "Epoch 132: loss 0.06325146555900574\n",
      "Epoch 133: loss 0.011198759078979492\n",
      "Epoch 134: loss 0.0628567636013031\n",
      "Epoch 135: loss 0.01363992691040039\n",
      "Epoch 136: loss 0.10436135530471802\n",
      "Epoch 137: loss 0.01979506015777588\n",
      "Epoch 138: loss 0.09746986627578735\n",
      "Epoch 139: loss 0.025911778211593628\n",
      "Epoch 140: loss 0.09062135219573975\n",
      "Epoch 141: loss 0.03199031949043274\n",
      "Epoch 142: loss 0.0838155746459961\n",
      "Epoch 143: loss 0.0380309522151947\n",
      "Epoch 144: loss 0.07705241441726685\n",
      "Epoch 145: loss 0.04403385519981384\n",
      "Epoch 146: loss 0.07033133506774902\n",
      "Epoch 147: loss 0.04999926686286926\n",
      "Epoch 148: loss 0.06365221738815308\n",
      "Epoch 149: loss 0.05592748522758484\n",
      "Epoch 150: loss 0.05701476335525513\n",
      "Epoch 151: loss 0.06181877851486206\n",
      "Epoch 152: loss 0.050418734550476074\n",
      "Epoch 153: loss 0.06767326593399048\n",
      "Epoch 154: loss 0.04386383295059204\n",
      "Epoch 155: loss 0.0734911561012268\n",
      "Epoch 156: loss 0.03734993934631348\n",
      "Epoch 157: loss 0.07927271723747253\n",
      "Epoch 158: loss 0.03400212526321411\n",
      "Epoch 159: loss 0.04053908586502075\n",
      "Epoch 160: loss 0.03378993272781372\n",
      "Epoch 161: loss 0.040286093950271606\n",
      "Epoch 162: loss 0.033579021692276\n",
      "Epoch 163: loss 0.040034741163253784\n",
      "Epoch 164: loss 0.033369481563568115\n",
      "Epoch 165: loss 0.03978487849235535\n",
      "Epoch 166: loss 0.0331612229347229\n",
      "Epoch 167: loss 0.03953665494918823\n",
      "Epoch 168: loss 0.03295430541038513\n",
      "Epoch 169: loss 0.03928995132446289\n",
      "Epoch 170: loss 0.032748639583587646\n",
      "Epoch 171: loss 0.03904476761817932\n",
      "Epoch 172: loss 0.03254431486129761\n",
      "Epoch 173: loss 0.03880113363265991\n",
      "Epoch 174: loss 0.032341182231903076\n",
      "Epoch 175: loss 0.039050936698913574\n",
      "Epoch 176: loss 0.07549065351486206\n",
      "Epoch 177: loss 0.045047491788864136\n",
      "Epoch 178: loss 0.06877928972244263\n",
      "Epoch 179: loss 0.05100664496421814\n",
      "Epoch 180: loss 0.06210988759994507\n",
      "Epoch 181: loss 0.05692863464355469\n",
      "Epoch 182: loss 0.05548197031021118\n",
      "Epoch 183: loss 0.06281355023384094\n",
      "Epoch 184: loss 0.04889565706253052\n",
      "Epoch 185: loss 0.06866183876991272\n",
      "Epoch 186: loss 0.04235023260116577\n",
      "Epoch 187: loss 0.07447361946105957\n",
      "Epoch 188: loss 0.03584575653076172\n",
      "Epoch 189: loss 0.08024901151657104\n",
      "Epoch 190: loss 0.029381930828094482\n",
      "Epoch 191: loss 0.08598843216896057\n",
      "Epoch 192: loss 0.02295839786529541\n",
      "Epoch 193: loss 0.09169206023216248\n",
      "Epoch 194: loss 0.016574859619140625\n",
      "Epoch 195: loss 0.09736013412475586\n",
      "Epoch 196: loss 0.015338510274887085\n",
      "Epoch 197: loss 0.05684497952461243\n",
      "Epoch 198: loss 0.015242815017700195\n",
      "Epoch 199: loss 0.05649027228355408\n",
      "Epoch 200: loss 0.015147656202316284\n",
      "Epoch 201: loss 0.05613777041435242\n",
      "Epoch 202: loss 0.01505306363105774\n",
      "Epoch 203: loss 0.055787473917007446\n",
      "Epoch 204: loss 0.014959156513214111\n",
      "Epoch 205: loss 0.05543932318687439\n",
      "Epoch 206: loss 0.01486581563949585\n",
      "Epoch 207: loss 0.05509337782859802\n",
      "Epoch 208: loss 0.014773011207580566\n",
      "Epoch 209: loss 0.05474966764450073\n",
      "Epoch 210: loss 0.014680802822113037\n",
      "Epoch 211: loss 0.054407984018325806\n",
      "Epoch 212: loss 0.014589190483093262\n",
      "Epoch 213: loss 0.05406847596168518\n",
      "Epoch 214: loss 0.01758044958114624\n",
      "Epoch 215: loss 0.09793668985366821\n",
      "Epoch 216: loss 0.02371084690093994\n",
      "Epoch 217: loss 0.09108537435531616\n",
      "Epoch 218: loss 0.029803156852722168\n",
      "Epoch 219: loss 0.08427673578262329\n",
      "Epoch 220: loss 0.035857439041137695\n",
      "Epoch 221: loss 0.07751059532165527\n",
      "Epoch 222: loss 0.041873812675476074\n",
      "Epoch 223: loss 0.07078665494918823\n",
      "Epoch 224: loss 0.04785281419754028\n",
      "Epoch 225: loss 0.06410479545593262\n",
      "Epoch 226: loss 0.053794413805007935\n",
      "Epoch 227: loss 0.05746448040008545\n",
      "Epoch 228: loss 0.05969896912574768\n",
      "Epoch 229: loss 0.05086565017700195\n",
      "Epoch 230: loss 0.06556662917137146\n",
      "Epoch 231: loss 0.04430806636810303\n",
      "Epoch 232: loss 0.07139775156974792\n",
      "Epoch 233: loss 0.03779125213623047\n",
      "Epoch 234: loss 0.07719248533248901\n",
      "Epoch 235: loss 0.03131520748138428\n",
      "Epoch 236: loss 0.08295106887817383\n",
      "Epoch 237: loss 0.024879515171051025\n",
      "Epoch 238: loss 0.08867371082305908\n",
      "Epoch 239: loss 0.018483996391296387\n",
      "Epoch 240: loss 0.09436053037643433\n",
      "Epoch 241: loss 0.012128353118896484\n",
      "Epoch 242: loss 0.10001194477081299\n",
      "Epoch 243: loss 0.005812466144561768\n",
      "Epoch 244: loss 0.105628103017807\n",
      "Epoch 245: loss 0.0024447739124298096\n",
      "Epoch 246: loss 0.06541019678115845\n",
      "Epoch 247: loss 0.0035829544067382812\n",
      "Epoch 248: loss 0.11297649145126343\n",
      "Epoch 249: loss 0.009800851345062256\n",
      "Epoch 250: loss 0.10603123903274536\n",
      "Epoch 251: loss 0.015979915857315063\n",
      "Epoch 252: loss 0.09912937879562378\n",
      "Epoch 253: loss 0.02212044596672058\n",
      "Epoch 254: loss 0.09227055311203003\n",
      "Epoch 255: loss 0.028222590684890747\n",
      "Epoch 256: loss 0.08545458316802979\n",
      "Epoch 257: loss 0.0342867374420166\n",
      "Epoch 258: loss 0.07868105173110962\n",
      "Epoch 259: loss 0.04031303524971008\n",
      "Epoch 260: loss 0.07194983959197998\n",
      "Epoch 261: loss 0.04630172252655029\n",
      "Epoch 262: loss 0.06526064872741699\n",
      "Epoch 263: loss 0.052253007888793945\n",
      "Epoch 264: loss 0.05861306190490723\n",
      "Epoch 265: loss 0.05816712975502014\n",
      "Epoch 266: loss 0.05200713872909546\n",
      "Epoch 267: loss 0.06404441595077515\n",
      "Epoch 268: loss 0.045442283153533936\n",
      "Epoch 269: loss 0.0698850154876709\n",
      "Epoch 270: loss 0.03891855478286743\n",
      "Epoch 271: loss 0.07568913698196411\n",
      "Epoch 272: loss 0.03243541717529297\n",
      "Epoch 273: loss 0.08145704865455627\n",
      "Epoch 274: loss 0.025992929935455322\n",
      "Epoch 275: loss 0.0871889591217041\n",
      "Epoch 276: loss 0.019590437412261963\n",
      "Epoch 277: loss 0.0928850769996643\n",
      "Epoch 278: loss 0.013227999210357666\n",
      "Epoch 279: loss 0.09854564070701599\n",
      "Epoch 280: loss 0.006905257701873779\n",
      "Epoch 281: loss 0.10417091846466064\n",
      "Epoch 282: loss 0.0049180686473846436\n",
      "Epoch 283: loss 0.06142681837081909\n",
      "Epoch 284: loss 0.004887431859970093\n",
      "Epoch 285: loss 0.06104347109794617\n",
      "Epoch 286: loss 0.00564044713973999\n",
      "Epoch 287: loss 0.11014950275421143\n",
      "Epoch 288: loss 0.011845499277114868\n",
      "Epoch 289: loss 0.10322189331054688\n",
      "Epoch 290: loss 0.018011897802352905\n",
      "Epoch 291: loss 0.09633749723434448\n",
      "Epoch 292: loss 0.024139732122421265\n",
      "Epoch 293: loss 0.08949607610702515\n",
      "Epoch 294: loss 0.0302293598651886\n",
      "Epoch 295: loss 0.08269733190536499\n",
      "Epoch 296: loss 0.03628101944923401\n",
      "Epoch 297: loss 0.07594102621078491\n",
      "Epoch 298: loss 0.042294859886169434\n",
      "Epoch 299: loss 0.06922674179077148\n",
      "Epoch 300: loss 0.048271238803863525\n",
      "Epoch 301: loss 0.06255453824996948\n",
      "Epoch 302: loss 0.05421024560928345\n",
      "Epoch 303: loss 0.0559239387512207\n",
      "Epoch 304: loss 0.06011223793029785\n",
      "Epoch 305: loss 0.04933464527130127\n",
      "Epoch 306: loss 0.06597739458084106\n",
      "Epoch 307: loss 0.04278653860092163\n",
      "Epoch 308: loss 0.07180589437484741\n",
      "Epoch 309: loss 0.03627932071685791\n",
      "Epoch 310: loss 0.07759809494018555\n",
      "Epoch 311: loss 0.029812633991241455\n",
      "Epoch 312: loss 0.08335414528846741\n",
      "Epoch 313: loss 0.023386359214782715\n",
      "Epoch 314: loss 0.08907419443130493\n",
      "Epoch 315: loss 0.017000257968902588\n",
      "Epoch 316: loss 0.09475857019424438\n",
      "Epoch 317: loss 0.015001624822616577\n",
      "Epoch 318: loss 0.05041429400444031\n",
      "Epoch 319: loss 0.014908045530319214\n",
      "Epoch 320: loss 0.050099700689315796\n",
      "Epoch 321: loss 0.014815002679824829\n",
      "Epoch 322: loss 0.049787044525146484\n",
      "Epoch 323: loss 0.014722555875778198\n",
      "Epoch 324: loss 0.04947632551193237\n",
      "Epoch 325: loss 0.014630705118179321\n",
      "Epoch 326: loss 0.04916760325431824\n",
      "Epoch 327: loss 0.014539450407028198\n",
      "Epoch 328: loss 0.048860758543014526\n",
      "Epoch 329: loss 0.014448702335357666\n",
      "Epoch 330: loss 0.048555850982666016\n",
      "Epoch 331: loss 0.014358609914779663\n",
      "Epoch 332: loss 0.0497073233127594\n",
      "Epoch 333: loss 0.06093078851699829\n",
      "Epoch 334: loss 0.05563732981681824\n",
      "Epoch 335: loss 0.054310381412506104\n",
      "Epoch 336: loss 0.06153041124343872\n",
      "Epoch 337: loss 0.047731220722198486\n",
      "Epoch 338: loss 0.0673866868019104\n",
      "Epoch 339: loss 0.04119306802749634\n",
      "Epoch 340: loss 0.07320642471313477\n",
      "Epoch 341: loss 0.03469580411911011\n",
      "Epoch 342: loss 0.07898983359336853\n",
      "Epoch 343: loss 0.028239011764526367\n",
      "Epoch 344: loss 0.08473721146583557\n",
      "Epoch 345: loss 0.022641301155090332\n",
      "Epoch 346: loss 0.06035029888153076\n",
      "Epoch 347: loss 0.05556529760360718\n",
      "Epoch 348: loss 0.05373340845108032\n",
      "Epoch 349: loss 0.06145879626274109\n",
      "Epoch 350: loss 0.04715794324874878\n",
      "Epoch 351: loss 0.0673154890537262\n",
      "Epoch 352: loss 0.04062342643737793\n",
      "Epoch 353: loss 0.07313567399978638\n",
      "Epoch 354: loss 0.03506147861480713\n",
      "Epoch 355: loss 0.029222548007965088\n",
      "Epoch 356: loss 0.03484266996383667\n",
      "Epoch 357: loss 0.02904018759727478\n",
      "Epoch 358: loss 0.03462526202201843\n",
      "Epoch 359: loss 0.028858989477157593\n",
      "Epoch 360: loss 0.03440919518470764\n",
      "Epoch 361: loss 0.028678864240646362\n",
      "Epoch 362: loss 0.0341944694519043\n",
      "Epoch 363: loss 0.028499901294708252\n",
      "Epoch 364: loss 0.033981114625930786\n",
      "Epoch 365: loss 0.0283220112323761\n",
      "Epoch 366: loss 0.033769071102142334\n",
      "Epoch 367: loss 0.028145283460617065\n",
      "Epoch 368: loss 0.03355833888053894\n",
      "Epoch 369: loss 0.027969688177108765\n",
      "Epoch 370: loss 0.03334894776344299\n",
      "Epoch 371: loss 0.030319154262542725\n",
      "Epoch 372: loss 0.0818777084350586\n",
      "Epoch 373: loss 0.03637021780014038\n",
      "Epoch 374: loss 0.0751265287399292\n",
      "Epoch 375: loss 0.04238346219062805\n",
      "Epoch 376: loss 0.068417489528656\n",
      "Epoch 377: loss 0.04835924506187439\n",
      "Epoch 378: loss 0.06175029277801514\n",
      "Epoch 379: loss 0.05429771542549133\n",
      "Epoch 380: loss 0.055124640464782715\n",
      "Epoch 381: loss 0.060199201107025146\n",
      "Epoch 382: loss 0.04854041337966919\n",
      "Epoch 383: loss 0.0660637617111206\n",
      "Epoch 384: loss 0.04199725389480591\n",
      "Epoch 385: loss 0.07189175486564636\n",
      "Epoch 386: loss 0.03549492359161377\n",
      "Epoch 387: loss 0.07768335938453674\n",
      "Epoch 388: loss 0.029033184051513672\n",
      "Epoch 389: loss 0.08343887329101562\n",
      "Epoch 390: loss 0.02261173725128174\n",
      "Epoch 391: loss 0.08915847539901733\n",
      "Epoch 392: loss 0.016230404376983643\n",
      "Epoch 393: loss 0.0948423445224762\n",
      "Epoch 394: loss 0.012305617332458496\n",
      "Epoch 395: loss 0.05038660764694214\n",
      "Epoch 396: loss 0.012228846549987793\n",
      "Epoch 397: loss 0.050072163343429565\n",
      "Epoch 398: loss 0.012152552604675293\n",
      "Epoch 399: loss 0.04975971579551697\n",
      "Epoch 400: loss 0.012076735496520996\n",
      "Epoch 401: loss 0.04944917559623718\n",
      "Epoch 402: loss 0.012001365423202515\n",
      "Epoch 403: loss 0.049140602350234985\n",
      "Epoch 404: loss 0.011926442384719849\n",
      "Epoch 405: loss 0.0488339364528656\n",
      "Epoch 406: loss 0.011851996183395386\n",
      "Epoch 407: loss 0.04852926731109619\n",
      "Epoch 408: loss 0.011839956045150757\n",
      "Epoch 409: loss 0.10211050510406494\n",
      "Epoch 410: loss 0.018006354570388794\n",
      "Epoch 411: loss 0.09523308277130127\n",
      "Epoch 412: loss 0.02413424849510193\n",
      "Epoch 413: loss 0.08839845657348633\n",
      "Epoch 414: loss 0.030223876237869263\n",
      "Epoch 415: loss 0.08160668611526489\n",
      "Epoch 416: loss 0.0362754762172699\n",
      "Epoch 417: loss 0.07485723495483398\n",
      "Epoch 418: loss 0.04228931665420532\n",
      "Epoch 419: loss 0.06814992427825928\n",
      "Epoch 420: loss 0.048265665769577026\n",
      "Epoch 421: loss 0.06148439645767212\n",
      "Epoch 422: loss 0.05420467257499695\n",
      "Epoch 423: loss 0.054860472679138184\n",
      "Epoch 424: loss 0.06010666489601135\n",
      "Epoch 425: loss 0.048277974128723145\n",
      "Epoch 426: loss 0.06597182154655457\n",
      "Epoch 427: loss 0.041736483573913574\n",
      "Epoch 428: loss 0.07180029153823853\n",
      "Epoch 429: loss 0.035235822200775146\n",
      "Epoch 430: loss 0.07759249210357666\n",
      "Epoch 431: loss 0.02877575159072876\n",
      "Epoch 432: loss 0.08334857225418091\n",
      "Epoch 433: loss 0.02235591411590576\n",
      "Epoch 434: loss 0.08906859159469604\n",
      "Epoch 435: loss 0.0159761905670166\n",
      "Epoch 436: loss 0.09475305676460266\n",
      "Epoch 437: loss 0.009636282920837402\n",
      "Epoch 438: loss 0.10040196776390076\n",
      "Epoch 439: loss 0.00461655855178833\n",
      "Epoch 440: loss 0.05579081177711487\n",
      "Epoch 441: loss 0.004587769508361816\n",
      "Epoch 442: loss 0.05544266104698181\n",
      "Epoch 443: loss 0.004559129476547241\n",
      "Epoch 444: loss 0.05509668588638306\n",
      "Epoch 445: loss 0.006015479564666748\n",
      "Epoch 446: loss 0.1084563136100769\n",
      "Epoch 447: loss 0.012218207120895386\n",
      "Epoch 448: loss 0.10153931379318237\n",
      "Epoch 449: loss 0.018382132053375244\n",
      "Epoch 450: loss 0.09466546773910522\n",
      "Epoch 451: loss 0.02450767159461975\n",
      "Epoch 452: loss 0.08783447742462158\n",
      "Epoch 453: loss 0.030594944953918457\n",
      "Epoch 454: loss 0.0810462236404419\n",
      "Epoch 455: loss 0.03664422035217285\n",
      "Epoch 456: loss 0.07430028915405273\n",
      "Epoch 457: loss 0.0426558256149292\n",
      "Epoch 458: loss 0.06759631633758545\n",
      "Epoch 459: loss 0.04862990975379944\n",
      "Epoch 460: loss 0.060934245586395264\n",
      "Epoch 461: loss 0.05456659197807312\n",
      "Epoch 462: loss 0.054313838481903076\n",
      "Epoch 463: loss 0.06046640872955322\n",
      "Epoch 464: loss 0.04773455858230591\n",
      "Epoch 465: loss 0.06632936000823975\n",
      "Epoch 466: loss 0.04119640588760376\n",
      "Epoch 467: loss 0.07215568423271179\n",
      "Epoch 468: loss 0.03469914197921753\n",
      "Epoch 469: loss 0.07794564962387085\n",
      "Epoch 470: loss 0.02824246883392334\n",
      "Epoch 471: loss 0.08369946479797363\n",
      "Epoch 472: loss 0.021825969219207764\n",
      "Epoch 473: loss 0.08941739797592163\n",
      "Epoch 474: loss 0.0154494047164917\n",
      "Epoch 475: loss 0.09509965777397156\n",
      "Epoch 476: loss 0.009112775325775146\n",
      "Epoch 477: loss 0.10074645280838013\n",
      "Epoch 478: loss 0.0028156638145446777\n",
      "Epoch 479: loss 0.10635802149772644\n",
      "Epoch 480: loss 0.003442108631134033\n",
      "Epoch 481: loss 0.11127883195877075\n",
      "Epoch 482: loss 0.009660840034484863\n",
      "Epoch 483: loss 0.1043441891670227\n",
      "Epoch 484: loss 0.015840768814086914\n",
      "Epoch 485: loss 0.09745287895202637\n",
      "Epoch 486: loss 0.0219821035861969\n",
      "Epoch 487: loss 0.09060454368591309\n",
      "Epoch 488: loss 0.02808523178100586\n",
      "Epoch 489: loss 0.08379888534545898\n",
      "Epoch 490: loss 0.034150153398513794\n",
      "Epoch 491: loss 0.07703578472137451\n",
      "Epoch 492: loss 0.04017725586891174\n",
      "Epoch 493: loss 0.07031482458114624\n",
      "Epoch 494: loss 0.04616677761077881\n",
      "Epoch 495: loss 0.06363582611083984\n",
      "Epoch 496: loss 0.05211895704269409\n",
      "Epoch 497: loss 0.05699843168258667\n",
      "Epoch 498: loss 0.05803397297859192\n",
      "Epoch 499: loss 0.05040252208709717\n",
      "Epoch 500: loss 0.06391209363937378\n",
      "Epoch 501: loss 0.043847739696502686\n",
      "Epoch 502: loss 0.06975346803665161\n",
      "Epoch 503: loss 0.037333905696868896\n",
      "Epoch 504: loss 0.07555845379829407\n",
      "Epoch 505: loss 0.03086066246032715\n",
      "Epoch 506: loss 0.0813271701335907\n",
      "Epoch 507: loss 0.02442789077758789\n",
      "Epoch 508: loss 0.0870598554611206\n",
      "Epoch 509: loss 0.01803523302078247\n",
      "Epoch 510: loss 0.09275683760643005\n",
      "Epoch 511: loss 0.011682391166687012\n",
      "Epoch 512: loss 0.0984182059764862\n",
      "Epoch 513: loss 0.005369365215301514\n",
      "Epoch 514: loss 0.10404431819915771\n",
      "Epoch 515: loss 0.0009086430072784424\n",
      "Epoch 516: loss 0.05791783332824707\n",
      "Epoch 517: loss 0.004028350114822388\n",
      "Epoch 518: loss 0.11056983470916748\n",
      "Epoch 519: loss 0.010243475437164307\n",
      "Epoch 520: loss 0.10363960266113281\n",
      "Epoch 521: loss 0.01641976833343506\n",
      "Epoch 522: loss 0.0967525839805603\n",
      "Epoch 523: loss 0.022557497024536133\n",
      "Epoch 524: loss 0.0899086594581604\n",
      "Epoch 525: loss 0.028656989336013794\n",
      "Epoch 526: loss 0.0831073522567749\n",
      "Epoch 527: loss 0.03471839427947998\n",
      "Epoch 528: loss 0.07634854316711426\n",
      "Epoch 529: loss 0.040741950273513794\n",
      "Epoch 530: loss 0.06963193416595459\n",
      "Epoch 531: loss 0.046727895736694336\n",
      "Epoch 532: loss 0.06295716762542725\n",
      "Epoch 533: loss 0.052676498889923096\n",
      "Epoch 534: loss 0.05632418394088745\n",
      "Epoch 535: loss 0.05858796834945679\n",
      "Epoch 536: loss 0.04973244667053223\n",
      "Epoch 537: loss 0.06446254253387451\n",
      "Epoch 538: loss 0.04318195581436157\n",
      "Epoch 539: loss 0.07030057907104492\n",
      "Epoch 540: loss 0.03667217493057251\n",
      "Epoch 541: loss 0.07610213756561279\n",
      "Epoch 542: loss 0.03020310401916504\n",
      "Epoch 543: loss 0.08186745643615723\n",
      "Epoch 544: loss 0.023774445056915283\n",
      "Epoch 545: loss 0.0875968337059021\n",
      "Epoch 546: loss 0.01738584041595459\n",
      "Epoch 547: loss 0.09329041838645935\n",
      "Epoch 548: loss 0.011037111282348633\n",
      "Epoch 549: loss 0.09894847869873047\n",
      "Epoch 550: loss 0.005087018013000488\n",
      "Epoch 551: loss 0.05357047915458679\n",
      "Epoch 552: loss 0.0050553083419799805\n",
      "Epoch 553: loss 0.053236156702041626\n",
      "Epoch 554: loss 0.005023777484893799\n",
      "Epoch 555: loss 0.0529039204120636\n",
      "Epoch 556: loss 0.004992395639419556\n",
      "Epoch 557: loss 0.05257377028465271\n",
      "Epoch 558: loss 0.00780978798866272\n",
      "Epoch 559: loss 0.10627496242523193\n",
      "Epoch 560: loss 0.014001339673995972\n",
      "Epoch 561: loss 0.09937149286270142\n",
      "Epoch 562: loss 0.020154207944869995\n",
      "Epoch 563: loss 0.0925111174583435\n",
      "Epoch 564: loss 0.02626866102218628\n",
      "Epoch 565: loss 0.08569365739822388\n",
      "Epoch 566: loss 0.032344937324523926\n",
      "Epoch 567: loss 0.0789186954498291\n",
      "Epoch 568: loss 0.03838333487510681\n",
      "Epoch 569: loss 0.0721859335899353\n",
      "Epoch 570: loss 0.04438409209251404\n",
      "Epoch 571: loss 0.06549513339996338\n",
      "Epoch 572: loss 0.05034738779067993\n",
      "Epoch 573: loss 0.05884629487991333\n",
      "Epoch 574: loss 0.05627349019050598\n",
      "Epoch 575: loss 0.05223870277404785\n",
      "Epoch 576: loss 0.0621626079082489\n",
      "Epoch 577: loss 0.04567253589630127\n",
      "Epoch 578: loss 0.06801488995552063\n",
      "Epoch 579: loss 0.039147257804870605\n",
      "Epoch 580: loss 0.07383072376251221\n",
      "Epoch 581: loss 0.032662808895111084\n",
      "Epoch 582: loss 0.0796101987361908\n",
      "Epoch 583: loss 0.026218712329864502\n",
      "Epoch 584: loss 0.08535367250442505\n",
      "Epoch 585: loss 0.01981484889984131\n",
      "Epoch 586: loss 0.0910612940788269\n",
      "Epoch 587: loss 0.013450920581817627\n",
      "Epoch 588: loss 0.09673330187797546\n",
      "Epoch 589: loss 0.010477036237716675\n",
      "Epoch 590: loss 0.04738730192184448\n",
      "Epoch 591: loss 0.010411649942398071\n",
      "Epoch 592: loss 0.04709157347679138\n",
      "Epoch 593: loss 0.010346740484237671\n",
      "Epoch 594: loss 0.04679769277572632\n",
      "Epoch 595: loss 0.010282129049301147\n",
      "Epoch 596: loss 0.04650565981864929\n",
      "Epoch 597: loss 0.010218054056167603\n",
      "Epoch 598: loss 0.046215444803237915\n",
      "Epoch 599: loss 0.010154247283935547\n",
      "Epoch 600: loss 0.0459270179271698\n",
      "Epoch 601: loss 0.011724799871444702\n",
      "Epoch 602: loss 0.10186076164245605\n",
      "Epoch 603: loss 0.01789185404777527\n",
      "Epoch 604: loss 0.09498488903045654\n",
      "Epoch 605: loss 0.02402043342590332\n",
      "Epoch 606: loss 0.08815199136734009\n",
      "Epoch 607: loss 0.030110716819763184\n",
      "Epoch 608: loss 0.08136171102523804\n",
      "Epoch 609: loss 0.036163002252578735\n",
      "Epoch 610: loss 0.07461380958557129\n",
      "Epoch 611: loss 0.04217752814292908\n",
      "Epoch 612: loss 0.06790804862976074\n",
      "Epoch 613: loss 0.048154592514038086\n",
      "Epoch 614: loss 0.06124395132064819\n",
      "Epoch 615: loss 0.05409431457519531\n",
      "Epoch 616: loss 0.05462157726287842\n",
      "Epoch 617: loss 0.059996962547302246\n",
      "Epoch 618: loss 0.048040568828582764\n",
      "Epoch 619: loss 0.06586283445358276\n",
      "Epoch 620: loss 0.04150044918060303\n",
      "Epoch 621: loss 0.07169204950332642\n",
      "Epoch 622: loss 0.03500121831893921\n",
      "Epoch 623: loss 0.07748490571975708\n",
      "Epoch 624: loss 0.028542637825012207\n",
      "Epoch 625: loss 0.0832415521144867\n",
      "Epoch 626: loss 0.02212435007095337\n",
      "Epoch 627: loss 0.08896234631538391\n",
      "Epoch 628: loss 0.0188157856464386\n",
      "Epoch 629: loss 0.03839591145515442\n",
      "Epoch 630: loss 0.018698394298553467\n",
      "Epoch 631: loss 0.03815630078315735\n",
      "Epoch 632: loss 0.018581748008728027\n",
      "Epoch 633: loss 0.03791818022727966\n",
      "Epoch 634: loss 0.018465757369995117\n",
      "Epoch 635: loss 0.03768157958984375\n",
      "Epoch 636: loss 0.018350571393966675\n",
      "Epoch 637: loss 0.03744637966156006\n",
      "Epoch 638: loss 0.01823607087135315\n",
      "Epoch 639: loss 0.03721272945404053\n",
      "Epoch 640: loss 0.018122315406799316\n",
      "Epoch 641: loss 0.03743100166320801\n",
      "Epoch 642: loss 0.07322245836257935\n",
      "Epoch 643: loss 0.043437659740448\n",
      "Epoch 644: loss 0.066525399684906\n",
      "Epoch 645: loss 0.04940682649612427\n",
      "Epoch 646: loss 0.059870004653930664\n",
      "Epoch 647: loss 0.05533871054649353\n",
      "Epoch 648: loss 0.05325615406036377\n",
      "Epoch 649: loss 0.061233580112457275\n",
      "Epoch 650: loss 0.046683669090270996\n",
      "Epoch 651: loss 0.0670916736125946\n",
      "Epoch 652: loss 0.04015207290649414\n",
      "Epoch 653: loss 0.072913259267807\n",
      "Epoch 654: loss 0.0336613655090332\n",
      "Epoch 655: loss 0.07869851589202881\n",
      "Epoch 656: loss 0.02721107006072998\n",
      "Epoch 657: loss 0.08444762229919434\n",
      "Epoch 658: loss 0.020801007747650146\n",
      "Epoch 659: loss 0.09016084671020508\n",
      "Epoch 660: loss 0.021453559398651123\n",
      "Epoch 661: loss 0.04792839288711548\n",
      "Epoch 662: loss 0.06597167253494263\n",
      "Epoch 663: loss 0.04138904809951782\n",
      "Epoch 664: loss 0.07180014252662659\n",
      "Epoch 665: loss 0.03489053249359131\n",
      "Epoch 666: loss 0.07759231328964233\n",
      "Epoch 667: loss 0.030198276042938232\n",
      "Epoch 668: loss 0.026404887437820435\n",
      "Epoch 669: loss 0.03000989556312561\n",
      "Epoch 670: loss 0.026240140199661255\n",
      "Epoch 671: loss 0.029822587966918945\n",
      "Epoch 672: loss 0.02607637643814087\n",
      "Epoch 673: loss 0.029636502265930176\n",
      "Epoch 674: loss 0.025913625955581665\n",
      "Epoch 675: loss 0.029451578855514526\n",
      "Epoch 676: loss 0.025751888751983643\n",
      "Epoch 677: loss 0.029267817735671997\n",
      "Epoch 678: loss 0.02559119462966919\n",
      "Epoch 679: loss 0.029085159301757812\n",
      "Epoch 680: loss 0.025431454181671143\n",
      "Epoch 681: loss 0.028903722763061523\n",
      "Epoch 682: loss 0.028391003608703613\n",
      "Epoch 683: loss 0.08327585458755493\n",
      "Epoch 684: loss 0.03445404767990112\n",
      "Epoch 685: loss 0.0765160322189331\n",
      "Epoch 686: loss 0.04047927260398865\n",
      "Epoch 687: loss 0.0697982907295227\n",
      "Epoch 688: loss 0.046466976404190063\n",
      "Epoch 689: loss 0.0631224513053894\n",
      "Epoch 690: loss 0.052417218685150146\n",
      "Epoch 691: loss 0.05648833513259888\n",
      "Epoch 692: loss 0.05833035707473755\n",
      "Epoch 693: loss 0.049895524978637695\n",
      "Epoch 694: loss 0.06420668959617615\n",
      "Epoch 695: loss 0.043343961238861084\n",
      "Epoch 696: loss 0.07004618644714355\n",
      "Epoch 697: loss 0.036833345890045166\n",
      "Epoch 698: loss 0.07584920525550842\n",
      "Epoch 699: loss 0.03036332130432129\n",
      "Epoch 700: loss 0.08161607384681702\n",
      "Epoch 701: loss 0.02393364906311035\n",
      "Epoch 702: loss 0.08734697103500366\n",
      "Epoch 703: loss 0.017544090747833252\n",
      "Epoch 704: loss 0.09304213523864746\n",
      "Epoch 705: loss 0.013955831527709961\n",
      "Epoch 706: loss 0.044146597385406494\n",
      "Epoch 707: loss 0.06941467523574829\n",
      "Epoch 708: loss 0.04019773006439209\n",
      "Epoch 709: loss 0.015599489212036133\n",
      "Epoch 710: loss 0.03994685411453247\n",
      "Epoch 711: loss 0.015502184629440308\n",
      "Epoch 712: loss 0.03969758749008179\n",
      "Epoch 713: loss 0.015405476093292236\n",
      "Epoch 714: loss 0.039449840784072876\n",
      "Epoch 715: loss 0.015309303998947144\n",
      "Epoch 716: loss 0.03920367360115051\n",
      "Epoch 717: loss 0.015213817358016968\n",
      "Epoch 718: loss 0.03895902633666992\n",
      "Epoch 719: loss 0.01511886715888977\n",
      "Epoch 720: loss 0.03871592879295349\n",
      "Epoch 721: loss 0.016595929861068726\n",
      "Epoch 722: loss 0.09646391868591309\n",
      "Epoch 723: loss 0.022732555866241455\n",
      "Epoch 724: loss 0.08962172269821167\n",
      "Epoch 725: loss 0.028831005096435547\n",
      "Epoch 726: loss 0.08282220363616943\n",
      "Epoch 727: loss 0.03489130735397339\n",
      "Epoch 728: loss 0.07606524229049683\n",
      "Epoch 729: loss 0.04091373085975647\n",
      "Epoch 730: loss 0.06935036182403564\n",
      "Epoch 731: loss 0.04689869284629822\n",
      "Epoch 732: loss 0.06267726421356201\n",
      "Epoch 733: loss 0.05284634232521057\n",
      "Epoch 734: loss 0.05604588985443115\n",
      "Epoch 735: loss 0.05875676870346069\n",
      "Epoch 736: loss 0.04945594072341919\n",
      "Epoch 737: loss 0.06463038921356201\n",
      "Epoch 738: loss 0.04290705919265747\n",
      "Epoch 739: loss 0.07046732306480408\n",
      "Epoch 740: loss 0.036399006843566895\n",
      "Epoch 741: loss 0.07626789808273315\n",
      "Epoch 742: loss 0.029931604862213135\n",
      "Epoch 743: loss 0.0820322036743164\n",
      "Epoch 744: loss 0.023504555225372314\n",
      "Epoch 745: loss 0.08776053786277771\n",
      "Epoch 746: loss 0.017117619514465332\n",
      "Epoch 747: loss 0.09345319867134094\n",
      "Epoch 748: loss 0.010770559310913086\n",
      "Epoch 749: loss 0.09911033511161804\n",
      "Epoch 750: loss 0.009693920612335205\n",
      "Epoch 751: loss 0.045114099979400635\n",
      "Epoch 752: loss 0.009633451700210571\n",
      "Epoch 753: loss 0.04483264684677124\n",
      "Epoch 754: loss 0.009573310613632202\n",
      "Epoch 755: loss 0.044552892446517944\n",
      "Epoch 756: loss 0.00951358675956726\n",
      "Epoch 757: loss 0.04427483677864075\n",
      "Epoch 758: loss 0.009454190731048584\n",
      "Epoch 759: loss 0.043998509645462036\n",
      "Epoch 760: loss 0.01104515790939331\n",
      "Epoch 761: loss 0.1027001142501831\n",
      "Epoch 762: loss 0.017216414213180542\n",
      "Epoch 763: loss 0.09581905603408813\n",
      "Epoch 764: loss 0.02334919571876526\n",
      "Epoch 765: loss 0.08898091316223145\n",
      "Epoch 766: loss 0.02944377064704895\n",
      "Epoch 767: loss 0.08218532800674438\n",
      "Epoch 768: loss 0.03550031781196594\n",
      "Epoch 769: loss 0.07543224096298218\n",
      "Epoch 770: loss 0.041518956422805786\n",
      "Epoch 771: loss 0.06872135400772095\n",
      "Epoch 772: loss 0.047500044107437134\n",
      "Epoch 773: loss 0.062052369117736816\n",
      "Epoch 774: loss 0.053443849086761475\n",
      "Epoch 775: loss 0.05542492866516113\n",
      "Epoch 776: loss 0.059350550174713135\n",
      "Epoch 777: loss 0.04883885383605957\n",
      "Epoch 778: loss 0.06522032618522644\n",
      "Epoch 779: loss 0.04229390621185303\n",
      "Epoch 780: loss 0.07105362415313721\n",
      "Epoch 781: loss 0.035789668560028076\n",
      "Epoch 782: loss 0.07685047388076782\n",
      "Epoch 783: loss 0.02932608127593994\n",
      "Epoch 784: loss 0.08261120319366455\n",
      "Epoch 785: loss 0.022902727127075195\n",
      "Epoch 786: loss 0.08833599090576172\n",
      "Epoch 787: loss 0.016519606113433838\n",
      "Epoch 788: loss 0.09402495622634888\n",
      "Epoch 789: loss 0.010176301002502441\n",
      "Epoch 790: loss 0.09967848658561707\n",
      "Epoch 791: loss 0.006245315074920654\n",
      "Epoch 792: loss 0.047908514738082886\n",
      "Epoch 793: loss 0.006206333637237549\n",
      "Epoch 794: loss 0.047609537839889526\n",
      "Epoch 795: loss 0.00616765022277832\n",
      "Epoch 796: loss 0.04731249809265137\n",
      "Epoch 797: loss 0.00612911581993103\n",
      "Epoch 798: loss 0.04701724648475647\n",
      "Epoch 799: loss 0.008571147918701172\n",
      "Epoch 800: loss 0.105499267578125\n",
      "Epoch 801: loss 0.014757931232452393\n",
      "Epoch 802: loss 0.09860068559646606\n",
      "Epoch 803: loss 0.020906031131744385\n",
      "Epoch 804: loss 0.09174507856369019\n",
      "Epoch 805: loss 0.0270158052444458\n",
      "Epoch 806: loss 0.08493244647979736\n",
      "Epoch 807: loss 0.03308743238449097\n",
      "Epoch 808: loss 0.07816219329833984\n",
      "Epoch 809: loss 0.03912124037742615\n",
      "Epoch 810: loss 0.0714341402053833\n",
      "Epoch 811: loss 0.045117348432540894\n",
      "Epoch 812: loss 0.06474816799163818\n",
      "Epoch 813: loss 0.05107596516609192\n",
      "Epoch 814: loss 0.05810403823852539\n",
      "Epoch 815: loss 0.05699741840362549\n",
      "Epoch 816: loss 0.05150115489959717\n",
      "Epoch 817: loss 0.06288200616836548\n",
      "Epoch 818: loss 0.044939637184143066\n",
      "Epoch 819: loss 0.06872984766960144\n",
      "Epoch 820: loss 0.03841894865036011\n",
      "Epoch 821: loss 0.07454115152359009\n",
      "Epoch 822: loss 0.03193897008895874\n",
      "Epoch 823: loss 0.08031624555587769\n",
      "Epoch 824: loss 0.025499343872070312\n",
      "Epoch 825: loss 0.08605533838272095\n",
      "Epoch 826: loss 0.01910001039505005\n",
      "Epoch 827: loss 0.0917586088180542\n",
      "Epoch 828: loss 0.012740552425384521\n",
      "Epoch 829: loss 0.09742626547813416\n",
      "Epoch 830: loss 0.00642085075378418\n",
      "Epoch 831: loss 0.10305842757225037\n",
      "Epoch 832: loss 0.005030781030654907\n",
      "Epoch 833: loss 0.04852175712585449\n",
      "Epoch 834: loss 0.004999339580535889\n",
      "Epoch 835: loss 0.048218995332717896\n",
      "Epoch 836: loss 0.006081759929656982\n",
      "Epoch 837: loss 0.10828155279159546\n",
      "Epoch 838: loss 0.012284010648727417\n",
      "Epoch 839: loss 0.10136562585830688\n",
      "Epoch 840: loss 0.018447667360305786\n",
      "Epoch 841: loss 0.0944928526878357\n",
      "Epoch 842: loss 0.024572759866714478\n",
      "Epoch 843: loss 0.08766293525695801\n",
      "Epoch 844: loss 0.030659615993499756\n",
      "Epoch 845: loss 0.0808756947517395\n",
      "Epoch 846: loss 0.036708563566207886\n",
      "Epoch 847: loss 0.07413071393966675\n",
      "Epoch 848: loss 0.04271981120109558\n",
      "Epoch 849: loss 0.0674278736114502\n",
      "Epoch 850: loss 0.04869338870048523\n",
      "Epoch 851: loss 0.06076699495315552\n",
      "Epoch 852: loss 0.05462971329689026\n",
      "Epoch 853: loss 0.054147541522979736\n",
      "Epoch 854: loss 0.060529083013534546\n",
      "Epoch 855: loss 0.047569334506988525\n",
      "Epoch 856: loss 0.06639164686203003\n",
      "Epoch 857: loss 0.041032254695892334\n",
      "Epoch 858: loss 0.07221755385398865\n",
      "Epoch 859: loss 0.03453594446182251\n",
      "Epoch 860: loss 0.07800713181495667\n",
      "Epoch 861: loss 0.028080284595489502\n",
      "Epoch 862: loss 0.08376052975654602\n",
      "Epoch 863: loss 0.021664857864379883\n",
      "Epoch 864: loss 0.08947804570198059\n",
      "Epoch 865: loss 0.015289366245269775\n",
      "Epoch 866: loss 0.09515997767448425\n",
      "Epoch 867: loss 0.008953750133514404\n",
      "Epoch 868: loss 0.100806325674057\n",
      "Epoch 869: loss 0.003934413194656372\n",
      "Epoch 870: loss 0.04969784617424011\n",
      "Epoch 871: loss 0.0039099156856536865\n",
      "Epoch 872: loss 0.04938766360282898\n",
      "Epoch 873: loss 0.0038854777812957764\n",
      "Epoch 874: loss 0.049079447984695435\n",
      "Epoch 875: loss 0.00669097900390625\n",
      "Epoch 876: loss 0.10762619972229004\n",
      "Epoch 877: loss 0.012889474630355835\n",
      "Epoch 878: loss 0.10071432590484619\n",
      "Epoch 879: loss 0.019049227237701416\n",
      "Epoch 880: loss 0.09384572505950928\n",
      "Epoch 881: loss 0.02517053484916687\n",
      "Epoch 882: loss 0.0870199203491211\n",
      "Epoch 883: loss 0.031253695487976074\n",
      "Epoch 884: loss 0.08023667335510254\n",
      "Epoch 885: loss 0.03729885816574097\n",
      "Epoch 886: loss 0.07349580526351929\n",
      "Epoch 887: loss 0.04330626130104065\n",
      "Epoch 888: loss 0.06679701805114746\n",
      "Epoch 889: loss 0.04927617311477661\n",
      "Epoch 890: loss 0.060140013694763184\n",
      "Epoch 891: loss 0.05520886182785034\n",
      "Epoch 892: loss 0.05352449417114258\n",
      "Epoch 893: loss 0.061104536056518555\n",
      "Epoch 894: loss 0.04695039987564087\n",
      "Epoch 895: loss 0.06696337461471558\n",
      "Epoch 896: loss 0.04041719436645508\n",
      "Epoch 897: loss 0.07278570532798767\n",
      "Epoch 898: loss 0.03392481803894043\n",
      "Epoch 899: loss 0.07857167720794678\n",
      "Epoch 900: loss 0.02747291326522827\n",
      "Epoch 901: loss 0.08432164788246155\n",
      "Epoch 902: loss 0.021061241626739502\n",
      "Epoch 903: loss 0.09003567695617676\n",
      "Epoch 904: loss 0.014689624309539795\n",
      "Epoch 905: loss 0.0957140326499939\n",
      "Epoch 906: loss 0.008357763290405273\n",
      "Epoch 907: loss 0.10135692358016968\n",
      "Epoch 908: loss 0.0029487907886505127\n",
      "Epoch 909: loss 0.05045294761657715\n",
      "Epoch 910: loss 0.002930372953414917\n",
      "Epoch 911: loss 0.050138115882873535\n",
      "Epoch 912: loss 0.004176735877990723\n",
      "Epoch 913: loss 0.11043357849121094\n",
      "Epoch 914: loss 0.010390877723693848\n",
      "Epoch 915: loss 0.10350418090820312\n",
      "Epoch 916: loss 0.016566306352615356\n",
      "Epoch 917: loss 0.09661799669265747\n",
      "Epoch 918: loss 0.022703170776367188\n",
      "Epoch 919: loss 0.08977490663528442\n",
      "Epoch 920: loss 0.028801709413528442\n",
      "Epoch 921: loss 0.08297443389892578\n",
      "Epoch 922: loss 0.03486227989196777\n",
      "Epoch 923: loss 0.07621639966964722\n",
      "Epoch 924: loss 0.04088491201400757\n",
      "Epoch 925: loss 0.0695006251335144\n",
      "Epoch 926: loss 0.04686999320983887\n",
      "Epoch 927: loss 0.06282657384872437\n",
      "Epoch 928: loss 0.05281782150268555\n",
      "Epoch 929: loss 0.056194305419921875\n",
      "Epoch 930: loss 0.05872851610183716\n",
      "Epoch 931: loss 0.04960334300994873\n",
      "Epoch 932: loss 0.06460225582122803\n",
      "Epoch 933: loss 0.04305356740951538\n",
      "Epoch 934: loss 0.07043930888175964\n",
      "Epoch 935: loss 0.036544740200042725\n",
      "Epoch 936: loss 0.07624000310897827\n",
      "Epoch 937: loss 0.030076444149017334\n",
      "Epoch 938: loss 0.08200451731681824\n",
      "Epoch 939: loss 0.023648500442504883\n",
      "Epoch 940: loss 0.08773300051689148\n",
      "Epoch 941: loss 0.01726067066192627\n",
      "Epoch 942: loss 0.09342578053474426\n",
      "Epoch 943: loss 0.010912716388702393\n",
      "Epoch 944: loss 0.09908300638198853\n",
      "Epoch 945: loss 0.00460439920425415\n",
      "Epoch 946: loss 0.10470494627952576\n",
      "Epoch 947: loss 0.0038808584213256836\n",
      "Epoch 948: loss 0.049289852380752563\n",
      "Epoch 949: loss 0.00477716326713562\n",
      "Epoch 950: loss 0.1097489595413208\n",
      "Epoch 951: loss 0.010987550020217896\n",
      "Epoch 952: loss 0.10282385349273682\n",
      "Epoch 953: loss 0.017159193754196167\n",
      "Epoch 954: loss 0.09594202041625977\n",
      "Epoch 955: loss 0.023292362689971924\n",
      "Epoch 956: loss 0.089103102684021\n",
      "Epoch 957: loss 0.029387205839157104\n",
      "Epoch 958: loss 0.0823068618774414\n",
      "Epoch 959: loss 0.03544408082962036\n",
      "Epoch 960: loss 0.07555299997329712\n",
      "Epoch 961: loss 0.04146307706832886\n",
      "Epoch 962: loss 0.06884133815765381\n",
      "Epoch 963: loss 0.047444552183151245\n",
      "Epoch 964: loss 0.0621715784072876\n",
      "Epoch 965: loss 0.05338871479034424\n",
      "Epoch 966: loss 0.055543363094329834\n",
      "Epoch 967: loss 0.05929577350616455\n",
      "Epoch 968: loss 0.04895657300949097\n",
      "Epoch 969: loss 0.06516590714454651\n",
      "Epoch 970: loss 0.042410850524902344\n",
      "Epoch 971: loss 0.07099944353103638\n",
      "Epoch 972: loss 0.035906076431274414\n",
      "Epoch 973: loss 0.07679662108421326\n",
      "Epoch 974: loss 0.02944183349609375\n",
      "Epoch 975: loss 0.08255761861801147\n",
      "Epoch 976: loss 0.0230177640914917\n",
      "Epoch 977: loss 0.08828270435333252\n",
      "Epoch 978: loss 0.01663386821746826\n",
      "Epoch 979: loss 0.09397202730178833\n",
      "Epoch 980: loss 0.010289907455444336\n",
      "Epoch 981: loss 0.0996258556842804\n",
      "Epoch 982: loss 0.003985404968261719\n",
      "Epoch 983: loss 0.1052444577217102\n",
      "Epoch 984: loss 0.0047505199909210205\n",
      "Epoch 985: loss 0.04850760102272034\n",
      "Epoch 986: loss 0.005388617515563965\n",
      "Epoch 987: loss 0.10905426740646362\n",
      "Epoch 988: loss 0.011595159769058228\n",
      "Epoch 989: loss 0.10213357210159302\n",
      "Epoch 990: loss 0.017762988805770874\n",
      "Epoch 991: loss 0.09525609016418457\n",
      "Epoch 992: loss 0.023892372846603394\n",
      "Epoch 993: loss 0.08842146396636963\n",
      "Epoch 994: loss 0.0299835205078125\n",
      "Epoch 995: loss 0.08162939548492432\n",
      "Epoch 996: loss 0.03603670001029968\n",
      "Epoch 997: loss 0.07487964630126953\n",
      "Epoch 998: loss 0.04205209016799927\n",
      "Epoch 999: loss 0.0681721568107605\n",
      "Epoch 1000: loss 0.048029959201812744\n"
     ]
    }
   ],
   "source": [
    "net = Net()\n",
    "net.init_layer1(torch.tensor([[0, 0]]), torch.tensor([0]))\n",
    "net.init_layer2(torch.tensor([[1]]), torch.tensor([0]))\n",
    "\n",
    "loss = net.train(x, y_true, eta=1e-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta1, theta2 = net.get_layer1().flatten().tolist()\n",
    "theta3 = net.get_layer2().flatten().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.47048945724394997, 1.3867693484556298)"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "theta1*theta3, theta2*theta3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[1.],\n",
       "         [1.]], grad_fn=<PowBackward0>),\n",
       " tensor([[0.8100],\n",
       "         [0.9506]], grad_fn=<PowBackward0>),\n",
       " tensor([[0.6526],\n",
       "         [0.9026]], grad_fn=<PowBackward0>),\n",
       " tensor([[0.5180],\n",
       "         [0.8543]], grad_fn=<PowBackward0>),\n",
       " tensor([[0.4020],\n",
       "         [0.8049]], grad_fn=<PowBackward0>),\n",
       " tensor([[0.3030],\n",
       "         [0.7540]], grad_fn=<PowBackward0>),\n",
       " tensor([[0.2204],\n",
       "         [0.7019]], grad_fn=<PowBackward0>),\n",
       " tensor([[0.1540],\n",
       "         [0.6491]], grad_fn=<PowBackward0>),\n",
       " tensor([[0.1027],\n",
       "         [0.5963]], grad_fn=<PowBackward0>),\n",
       " tensor([[0.0650],\n",
       "         [0.5443]], grad_fn=<PowBackward0>),\n",
       " tensor([[0.0387],\n",
       "         [0.4939]], grad_fn=<PowBackward0>),\n",
       " tensor([[0.0214],\n",
       "         [0.4456]], grad_fn=<PowBackward0>),\n",
       " tensor([[0.0108],\n",
       "         [0.4000]], grad_fn=<PowBackward0>),\n",
       " tensor([[0.0047],\n",
       "         [0.3574]], grad_fn=<PowBackward0>),\n",
       " tensor([[0.0016],\n",
       "         [0.3179]], grad_fn=<PowBackward0>),\n",
       " tensor([[0.0003],\n",
       "         [0.2816]], grad_fn=<PowBackward0>),\n",
       " tensor([[4.7394e-07],\n",
       "         [2.4853e-01]], grad_fn=<PowBackward0>),\n",
       " tensor([[1.5012e-04],\n",
       "         [2.1846e-01]], grad_fn=<PowBackward0>),\n",
       " tensor([[0.0005],\n",
       "         [0.1913]], grad_fn=<PowBackward0>),\n",
       " tensor([[0.0008],\n",
       "         [0.1669]], grad_fn=<PowBackward0>),\n",
       " tensor([[0.0011],\n",
       "         [0.1451]], grad_fn=<PowBackward0>),\n",
       " tensor([[0.0013],\n",
       "         [0.1257]], grad_fn=<PowBackward0>),\n",
       " tensor([[0.0013],\n",
       "         [0.1084]], grad_fn=<PowBackward0>),\n",
       " tensor([[0.0014],\n",
       "         [0.0932]], grad_fn=<PowBackward0>),\n",
       " tensor([[0.0013],\n",
       "         [0.0799]], grad_fn=<PowBackward0>),\n",
       " tensor([[0.0013],\n",
       "         [0.0682]], grad_fn=<PowBackward0>),\n",
       " tensor([[0.0012],\n",
       "         [0.0580]], grad_fn=<PowBackward0>),\n",
       " tensor([[0.0011],\n",
       "         [0.0492]], grad_fn=<PowBackward0>),\n",
       " tensor([[0.0009],\n",
       "         [0.0416]], grad_fn=<PowBackward0>),\n",
       " tensor([[0.0008],\n",
       "         [0.0351]], grad_fn=<PowBackward0>),\n",
       " tensor([[0.0007],\n",
       "         [0.0295]], grad_fn=<PowBackward0>),\n",
       " tensor([[0.0006],\n",
       "         [0.0247]], grad_fn=<PowBackward0>),\n",
       " tensor([[0.0005],\n",
       "         [0.0207]], grad_fn=<PowBackward0>),\n",
       " tensor([[0.0005],\n",
       "         [0.0173]], grad_fn=<PowBackward0>),\n",
       " tensor([[0.0004],\n",
       "         [0.0144]], grad_fn=<PowBackward0>),\n",
       " tensor([[0.0003],\n",
       "         [0.0119]], grad_fn=<PowBackward0>),\n",
       " tensor([[0.0003],\n",
       "         [0.0099]], grad_fn=<PowBackward0>),\n",
       " tensor([[0.0002],\n",
       "         [0.0082]], grad_fn=<PowBackward0>),\n",
       " tensor([[0.0002],\n",
       "         [0.0068]], grad_fn=<PowBackward0>),\n",
       " tensor([[0.0002],\n",
       "         [0.0056]], grad_fn=<PowBackward0>),\n",
       " tensor([[0.0001],\n",
       "         [0.0046]], grad_fn=<PowBackward0>),\n",
       " tensor([[0.0001],\n",
       "         [0.0038]], grad_fn=<PowBackward0>),\n",
       " tensor([[8.9581e-05],\n",
       "         [3.1108e-03]], grad_fn=<PowBackward0>),\n",
       " tensor([[7.3794e-05],\n",
       "         [2.5538e-03]], grad_fn=<PowBackward0>),\n",
       " tensor([[6.0693e-05],\n",
       "         [2.0944e-03]], grad_fn=<PowBackward0>),\n",
       " tensor([[4.9851e-05],\n",
       "         [1.7162e-03]], grad_fn=<PowBackward0>),\n",
       " tensor([[4.0899e-05],\n",
       "         [1.4050e-03]], grad_fn=<PowBackward0>),\n",
       " tensor([[3.3516e-05],\n",
       "         [1.1494e-03]], grad_fn=<PowBackward0>),\n",
       " tensor([[2.7441e-05],\n",
       "         [9.3969e-04]], grad_fn=<PowBackward0>),\n",
       " tensor([[2.2447e-05],\n",
       "         [7.6773e-04]], grad_fn=<PowBackward0>),\n",
       " tensor([[1.8350e-05],\n",
       "         [6.2689e-04]], grad_fn=<PowBackward0>),\n",
       " tensor([[1.4989e-05],\n",
       "         [5.1162e-04]], grad_fn=<PowBackward0>),\n",
       " tensor([[1.2237e-05],\n",
       "         [4.1735e-04]], grad_fn=<PowBackward0>),\n",
       " tensor([[9.9848e-06],\n",
       "         [3.4030e-04]], grad_fn=<PowBackward0>),\n",
       " tensor([[8.1425e-06],\n",
       "         [2.7738e-04]], grad_fn=<PowBackward0>),\n",
       " tensor([[6.6382e-06],\n",
       "         [2.2601e-04]], grad_fn=<PowBackward0>),\n",
       " tensor([[5.4092e-06],\n",
       "         [1.8409e-04]], grad_fn=<PowBackward0>),\n",
       " tensor([[4.4065e-06],\n",
       "         [1.4991e-04]], grad_fn=<PowBackward0>),\n",
       " tensor([[3.5881e-06],\n",
       "         [1.2204e-04]], grad_fn=<PowBackward0>),\n",
       " tensor([[2.9214e-06],\n",
       "         [9.9327e-05]], grad_fn=<PowBackward0>),\n",
       " tensor([[2.3777e-06],\n",
       "         [8.0827e-05]], grad_fn=<PowBackward0>),\n",
       " tensor([[1.9350e-06],\n",
       "         [6.5759e-05]], grad_fn=<PowBackward0>),\n",
       " tensor([[1.5742e-06],\n",
       "         [5.3491e-05]], grad_fn=<PowBackward0>),\n",
       " tensor([[1.2806e-06],\n",
       "         [4.3505e-05]], grad_fn=<PowBackward0>),\n",
       " tensor([[1.0413e-06],\n",
       "         [3.5378e-05]], grad_fn=<PowBackward0>),\n",
       " tensor([[8.4672e-07],\n",
       "         [2.8765e-05]], grad_fn=<PowBackward0>),\n",
       " tensor([[6.8859e-07],\n",
       "         [2.3387e-05]], grad_fn=<PowBackward0>),\n",
       " tensor([[5.5992e-07],\n",
       "         [1.9012e-05]], grad_fn=<PowBackward0>),\n",
       " tensor([[4.5509e-07],\n",
       "         [1.5454e-05]], grad_fn=<PowBackward0>),\n",
       " tensor([[3.7006e-07],\n",
       "         [1.2560e-05]], grad_fn=<PowBackward0>),\n",
       " tensor([[3.0070e-07],\n",
       "         [1.0208e-05]], grad_fn=<PowBackward0>),\n",
       " tensor([[2.4439e-07],\n",
       "         [8.2956e-06]], grad_fn=<PowBackward0>),\n",
       " tensor([[1.9867e-07],\n",
       "         [6.7412e-06]], grad_fn=<PowBackward0>),\n",
       " tensor([[1.6139e-07],\n",
       "         [5.4776e-06]], grad_fn=<PowBackward0>),\n",
       " tensor([[1.3107e-07],\n",
       "         [4.4509e-06]], grad_fn=<PowBackward0>),\n",
       " tensor([[1.0653e-07],\n",
       "         [3.6164e-06]], grad_fn=<PowBackward0>),\n",
       " tensor([[8.6559e-08],\n",
       "         [2.9382e-06]], grad_fn=<PowBackward0>),\n",
       " tensor([[7.0353e-08],\n",
       "         [2.3869e-06]], grad_fn=<PowBackward0>),\n",
       " tensor([[5.7128e-08],\n",
       "         [1.9390e-06]], grad_fn=<PowBackward0>)]"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
