# Deep_Networks

## Experiments 

1. Understanding the roles of Activations in Deep Neural Networks 

The hypothesis was to identify number of active subnetworks in DNNs, DLGNs, DLGNSFs and thier coorelation with the performance of the network. We tried to restrict the number of active subnetworks by imposing constraints on how the path functions are activated (fixed logic, random logic, random logic, etc.). We conducted studies on both MLP and CNN based networks. 

2. Over-parameterisation on the implicit acceleration of training

The idea was to understand the dynamics of paths - contribution of a particular path to the final output and number of paths that change directions during training. 

3.  Effect of Initialization on acceleration

The hypothesis is to understand how initialization affects alignment of layers.

4. Committees 




## Papers Read
- Are All Layers Created Equal? (Zhang et. al.)
- Complexity of Linear Regions in Deep Networks (Hanin et. al.)
- Gradient Descent Quantizes ReLU Network Features (Maennel et. al.)
- Deconstructing Lottery Tickets: Zeros, Signs, and the Supermask (Zhou et. al.)
- The Lottery Ticket Hypothesis: Finding Sparse, Trainable Neural Networks (Frankle et. al.)
- On the Number of Linear Regions of Deep Neural Networks (Montufar et. al.)
- Is Deeper Better only when Shallow is Good? (Malach et. al.)
- Predicting unreliable predictions by shattering a neural network
- Why do tree-based models still outperform deep learning on tabular data? (Grinsztajn et. al.)
- Neural Path Features and Neural Path Kernel:  Understanding the role of gates in deep learning (Chandrashekar et. al.)
- Gradient descent aligns the layers of deep linear networks (Telgarsky et. al.)
- On the Optimization of Deep Networks: Implicit Acceleration by Overparameterization (Arora et. al.)