# Deep_Networks

Experiments 

1. Understanding the roles of Activations in Deep Neural Networks 
Tested the performance DNN vs DGN vs DLGN vs DLGNSF to characterize the role of activations in deep neural networks.

2. Over-parameterisation on the implicit acceleration of training 

3. Initialization

4. Committees 



### Papers Read
- Are All Layers Created Equal? (Zhang et. al.)
- Complexity of Linear Regions in Deep Networks (Hanin et. al.)
- Gradient Descent Quantizes ReLU Network Features (Maennel et. al.)
- Deconstructing Lottery Tickets: Zeros, Signs, and the Supermask (Zhou et. al.)
- The Lottery Ticket Hypothesis: Finding Sparse, Trainable Neural Networks (Frankle et. al.)
- On the Number of Linear Regions of Deep Neural Networks (Montufar et. al.)
- Is Deeper Better only when Shallow is Good? (Malach et. al.)
- Predicting unreliable predictions by shattering a neural network
- Why do tree-based models still outperform deep learning on tabular data? (Grinsztajn et. al.)
- Neural Path Features and Neural Path Kernel:  Understanding the role of gates in deep learning (Chandrashekar et. al.)
- Gradient descent aligns the layers of deep linear networks (Telgarsky et. al.)
- On the Optimization of Deep Networks: Implicit Acceleration by Overparameterization (Arora et. al.)