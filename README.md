# Deep_Networks

## Experiments 

1. Understanding the roles of Activations in Deep Neural Networks 

The hypothesis was to identify number of active subnetworks in DNNs, DLGNs, DLGNSFs and thier coorelation with the performance of the network. We tried to restrict the number of active subnetworks by imposing constraints on how the path functions are activated (fixed logic, random logic, random logic, etc.). We conducted studies on both MLP and CNN based networks. 

2. Over-parameterisation on the implicit acceleration of training

The idea was to understand the dynamics of paths - contribution of a particular path to the final output and number of paths that change directions during training. 

3.  Effect of Initialization on acceleration

The hypothesis is to understand how initialization affects alignment of layers.

4. Committees 




## Papers Read
- Are All Layers Created Equal? (Journal of Machine Learning Research 23 (2022); Chiyuan Zhang, Samy Bengio, Yoram Singer)
- Complexity of Linear Regions in Deep Networks (Proceedings of Machine Learning Research (2019); Boris Hanin, David Rolnick)
- Gradient Descent Quantizes ReLU Network Features (arXiv:1803.08367; Hartmut Maennel, Olivier Bousquet, Sylvain Gelly)
- Deconstructing Lottery Tickets: Zeros, Signs, and the Supermask (Neural Information Processing Systems (NeurIPS 2019); Hattie Zhou, Janice Lan, Rosanne Liu, Jason Yosinski)
- The Lottery Ticket Hypothesis: Finding Sparse, Trainable Neural Networks (arXiv:1803.03635; Jonathan Frankle, Michael Carbin)
- On the Number of Linear Regions of Deep Neural Networks (Neural Information Processing Systems (NeurIPS 2014); Guido Montúfar, Razvan Pascanu, Kyunghyun Cho, Yoshua Bengio)
- Is Deeper Better only when Shallow is Good? (arXiv:1903.03488; Eran Malach, Shai Shalev-Shwartz)
- Predicting unreliable predictions by shattering a neural network (arXiv:2106.xxxxx 2021; Xu Ji, Razvan Pascanu, Devon Hjelm, Andrea Vedaldi, Balaji Lakshminarayanan, Yoshua Bengio)
- Why do tree-based models still outperform deep learning on tabular data? (Neural Information Processing Systems (NeurIPS 2022); Léo Grinsztajn, Edouard Oyallon, Gaël Varoquaux)
- Neural Path Features and Neural Path Kernel:  Understanding the role of gates in deep learning (Neural Information Processing Systems (NeurIPS 2020); Chandrashekar Lakshminarayanan, Amit Vikram Singh)
- Gradient descent aligns the layers of deep linear networks (arXiv:1810.02032; Ziwei Ji, Matus Telgarsky)
- On the Optimization of Deep Networks: Implicit Acceleration by Overparameterization (Neural Information Processing Systems (NeurIPS 2018); Sanjeev Arora, Nadav Cohen, Elad Hazan)
- Explicitising the implicit interpretability of Deep Neural Network via Duality (arXiv:2203.16455, Chandrashekar Lakshminarayanan, Amit Vikram Singh, Arun Rajkumar)